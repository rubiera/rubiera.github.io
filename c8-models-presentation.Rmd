---
title: "Machine Learning Project"
author: "Antonio Rubiera"
date: "11/28/2019"
output: 
  ioslides_presentation: 
    smaller: yes
---

```{r presentation setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary of this Analysis

In this analysis, we evaluate the data used by Velloso et al. in their paper "Qualitative Activity Recognition of Weight Lifting Exercises" [ACM SIGCHI 2013]. The data was collected for six young men performing weight-lifting exercises with a light dumbbell (1.25 KG) using five pre-determined sequences (named as the variable 'classe' in the dataset). Sequence A is the correct sequence, and sequences B, C, D, and E are variations of the men performing the weigth-lifting exercise incorrectly. Sequences B, C, D, and E are specific ways to perform the weigth-lifting exercise incorrectly, and this means that the four wrong sequences should be just as separable from each other as they are from A.

## Summary of this Analysis

We use a specific seed (22) to ensure our data runs are reproducible. We perform exploratory data analysis (EDA), caret model selection, and feature selection in an appendix. The steps relegated to an appendix are as follows:

- Explore the unaggregated training time series data to get a feel for what movements were performed for each sequence, as captured by the classe variable. We plot a small sample of these.
- Explore the aggregated training time series data after selecting only the rows containing these fileds, and removing fields that contain no data, all zeroes and/or all NA values.

## Summary of this Analysis

- For the aggregated time bin training data, we generate the model accuracy contribution of each feature using the Recursive Feature Elimination (RFE) algorithm. We come up with the features with greater than or equal to 0.75 correlation as features that can be excluded from the classification training algorithms, but none of these fields are found in the test data. Therefore, this selection criteria is not useful in our analysis.

- For the aggregated time bin training data, and using all predictors found in the training data, we apply four models (Decision Tree, Random Forest, Bagged Trees and SVMpoly-short for Support Vector Machines with polynomial kernel) using the caret package to asses the within sample accuracy of predicting the weight-lifting activities using classe as our response variable. We resample from within the training data to obtain confusion matrix results for each caret model. 

## Summary of this Analysis

- For the unaggregated time bin training data, we generate the model accuracy contribution of each feature using the RFE algorithm. We come up with the features with greater than or equal to 0.75 correlation as features that can be excluded from the classification training algorithms. Since some of these fields we want to exclude are found in the test data, we use this knowledge. 

- Finally, within the appendix, and for the unaggregated time bin training data, using all predictors found in the training data, we apply the four models we found to have the highest accuracy and kappa measures (Decision Tree, Random Forest, Bagged Trees and SVMpoly), using the caret package to asses the within sample accuracy of predicting the weight-lifting activities using classe as our response variable. We resample here as well from within the training data to obtain confusion matrix results for each caret model.

## Summary of this Analysis

We estimate in-sample error using the model's accuracy when resampling from within the training data, and out-of-sample error using the model's accuracy when comparing the training data to the test data. All of the percent accuracy numbers quoted in this summary are for a Random Forest model, and we have the same results for three other models (Decision Tree, Bagged Trees and SVMpoly) in the body of the analysis. The estimates for in-sample error using the model's accuracy are in the appendix, and the estimates for out-of-sample error using the model's accuracy are in the three cycles of analysis in the main body of this work.

## Summary of this Analysis

From the appendix, our in-sample error results for the Random Forest model are:

- 77.5 percent when using only the time-aggregated predictors.
- 84.8 percent when using only the unaggregated predictors, for all fields
- 81.8 percent when using only the unaggregated predictors, for all fields minus the highly correlated fields.

After we perform all of the steps previously outlined and contained here in an appendix, we have four caret models we want to apply on the test data in two cycles:

- In the first cycle, we use all predictors. We obtain predictions for classe for the 20 events in the test data. We estimate that the out of sample error will be close to the in-sample error (approximately 84.8 percent mean accuracy for a Random Forest model using the unaggregate time data). We find the mean accuracy for a Random Forest model to be 79.3 percent using all predictors.

## Summary of this Analysis

- In the second and cycle, we use only the predictors that have less than 0.75 correlation, based on our cross-validation analysis described in the appendix. We expect the out of sample error will be close to the in-sample error (approximately 81.8 percent mean accuracy for a Random Forest model using the unaggregate time data). We find the mean accuracy for a Random Forest model to be 76.0 percent using only the least-correlated predictors. We expected this accuracy to be higher than the accuracy when using all predictors.

- In the third and last cycle, We find the mean accuracy for a Random Forest model to be 77.3 using only the top 20 least-correlated predictors. We expected this accuracy to be higher than the accuracy when using all predictors. We subtract one predictor at a time for four additional accuracy measurements and find that the accuracy is 78.8 percent minus one predictor, 76.8 percent minus two predictors, 76.8 percent minus three predictors, and 77.3 percent minus four predictors.

## Summary of this Analysis

From these three cycles of analysis, our out-of-sample error results for the Random Forest model are:

- 79.3 percent when using only the unaggregated predictors, for all fields
- 76.0 percent when using only the unaggregated predictors, for all fields minus the highly correlated fields.
-Within the range between 76.8 percent and 78.8 percent when using only the top 20 unaggregated predictors, and subtracting one field at a time based on the accuracy results of the top 20 model four times.

We present our conclusion here for the benefit of the Coursera graders. The rest of this analysis has all of the supporting work for this conclusion. We find the in-sample error to be slightly lower than the out-of-sample error. We also find all of the error measurements, using model accuracy as a proxy, to be in the range from the higher 70s percents to the lower 80s percents. Our predictor selection did not change the accuracy results significantly. We attribute this result to a high level of noise in the data, and many missing fields. 
 

```{r presentation model-library-seed, warning=FALSE, message=FALSE, comment="", echo=TRUE}
library(tidyverse)
library(caret)
library(knitr)
set.seed(22)
```

## Time Series (Not Aggregated over Time) Data

The training dataset has two types of data: a time series dataset, and time window measurements of this time series data. Here is the time series data. The measurements in this data for sensors located at the belt, arm (glove), forearm, and dumbbell are:

- The Euler angles (roll, pitch, yaw)
- Gyroscope measurement (x,y,z)
- Magnetometer measurement (x,y,z)
- Acceleration measurement (x,y,z)

***


```{r presentation data-time, warning=FALSE, message=FALSE, comment="", echo=TRUE}
training_all <- read.csv("./data/pml-training.csv")
training_all_data <- select(training_all, c("user_name",
                            "raw_timestamp_part_1","raw_timestamp_part_2",
                            "cvtd_timestamp","new_window","num_window","roll_belt",
                            "pitch_belt","yaw_belt","total_accel_belt","gyros_belt_x",
                            "gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y",
                            "accel_belt_z","magnet_belt_x","magnet_belt_y",
                            "magnet_belt_z","roll_arm","pitch_arm","yaw_arm",
                            "total_accel_arm","gyros_arm_x","gyros_arm_y",
                            "gyros_arm_z","accel_arm_x","accel_arm_y","accel_arm_z",
                            "magnet_arm_x","magnet_arm_y","magnet_arm_z","roll_dumbbell",
                            "pitch_dumbbell","yaw_dumbbell","total_accel_dumbbell",
                            "gyros_dumbbell_x","gyros_dumbbell_y","gyros_dumbbell_z",
                            "accel_dumbbell_x","accel_dumbbell_y","accel_dumbbell_z",                                                    "magnet_dumbbell_x","magnet_dumbbell_y","magnet_dumbbell_z",
                            "roll_forearm","pitch_forearm","yaw_forearm",
                            "total_accel_forearm","gyros_forearm_x",
                            "gyros_forearm_y","gyros_forearm_z","accel_forearm_x",
                            "accel_forearm_y","accel_forearm_z","magnet_forearm_x",
                            "magnet_forearm_y","magnet_forearm_z","classe"))
```

*** 

Here are the counts by user_name and classe for the time series data.

```{r presentation data-kable-1, warning=FALSE, message=FALSE, comment="", echo=TRUE}
kable(table(training_all_data$user_name, training_all_data$classe))
```

## Predicting Classe for the Test Data for all Predictors (First Analysis Cycle)

We relegate the explotary data analysis, and running various caret models on resampled training data to an appendix. From this analysis, we determine that four types of models should yield the most accuate results:

- Random Forests (method="rf" in caret train function)
- Decision Trees (method="C5.0" in caret train function)
- Bagged Trees (method="treebag" in caret train function)
- SVM Poly, short for Least Squares Support Vector Machines with Polynomial Kernel  (method="svmPoly" in caret train function)

We run on these four models to make our predictions for which classe was performed for the twenty data records collected in the test data. We first need to select the complete observations fields in the test data to match fields in the training data. We save a copy of all of the non-aggregated columns in the training data (training_nonblanks_cv) for a cross-validation analysis enclosed here as an appendix.

*** 

```{r presentation predict-1, warning=FALSE, message=FALSE, comment="", echo=TRUE}
training_all_clean <- read.csv("./data/pml-training.csv",
                               na.strings = c("NA","NaN","","#DIV/0!"))
training_nonblanks_clean <- filter(training_all_clean, kurtosis_roll_belt != "")
training_nonblanks_clean <- mutate(training_nonblanks_clean, 
                                   rel_time = as.numeric(paste(raw_timestamp_part_1,
                                                               raw_timestamp_part_2,sep=".")))
```

***

```{r presentation 2 predict-1, warning=FALSE, message=FALSE, comment="", echo=TRUE}
training_nonblanks_clean <- select(training_nonblanks_clean, 
                c("roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", 
                "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x",
                "accel_belt_y", "accel_belt_z", "magnet_belt_x", 
                "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm",
                "accel_belt_y", "accel_belt_z", "magnet_belt_x", 
                "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell",
                "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y",
                "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", 
                "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y",
                "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm",
                "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z", 
                "accel_forearm_x", "accel_forearm_y",
                "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", 
                "magnet_forearm_z", "classe"))
```

***

```{r presentation 3 predict-1, warning=FALSE, message=FALSE, comment="", echo=TRUE}
training_blanks_cv <- select(training_all, c("roll_belt",
                      "pitch_belt","yaw_belt","total_accel_belt","gyros_belt_x",
                      "gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y",
                      "accel_belt_z","magnet_belt_x","magnet_belt_y",
                      "magnet_belt_z","roll_arm","pitch_arm","yaw_arm",
                      "total_accel_arm","gyros_arm_x","gyros_arm_y",
                      "gyros_arm_z","accel_arm_x","accel_arm_y","accel_arm_z",
                      "magnet_arm_x","magnet_arm_y","magnet_arm_z","roll_dumbbell",
                      "pitch_dumbbell","yaw_dumbbell","total_accel_dumbbell",
                      "gyros_dumbbell_x","gyros_dumbbell_y","gyros_dumbbell_z",
                      "accel_dumbbell_x","accel_dumbbell_y","accel_dumbbell_z",
                      "magnet_dumbbell_x","magnet_dumbbell_y","magnet_dumbbell_z",
                      "roll_forearm","pitch_forearm","yaw_forearm",
                      "total_accel_forearm","gyros_forearm_x",
                      "gyros_forearm_y","gyros_forearm_z","accel_forearm_x",
                      "accel_forearm_y","accel_forearm_z","magnet_forearm_x",
                      "magnet_forearm_y","magnet_forearm_z","classe"))

```

*** 

Time Series (Not Aggregated over Time) Data
Here is the selection of complete observation fields in the test data.

```{r presentation predict-2, warning=FALSE, message=FALSE, comment="", echo=TRUE}
testing_all_clean <- read.csv("./data/pml-testing.csv")
testing_nonblanks_clean <- mutate(testing_all_clean, 
                                  rel_time = as.numeric(paste(raw_timestamp_part_1,
                                                              raw_timestamp_part_2,sep=".")))
```

***

```{r presentation 2 predict-2, warning=FALSE, message=FALSE, comment="", echo=TRUE}
testing_nonblanks_clean <- select(testing_nonblanks_clean, 
                        c("roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", 
                        "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "accel_belt_x",
                        "accel_belt_y", "accel_belt_z", "magnet_belt_x", 
                        "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm",
                        "accel_belt_y", "accel_belt_z", "magnet_belt_x", 
                        "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm",
                        "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell",
                        "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y",
                        "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", 
                        "accel_dumbbell_z", "magnet_dumbbell_x", "magnet_dumbbell_y",
                        "magnet_dumbbell_z", "roll_forearm", "pitch_forearm", "yaw_forearm",
                        "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z", 
                        "accel_forearm_x", "accel_forearm_y",
                        "accel_forearm_z", "magnet_forearm_x", "magnet_forearm_y", 
                         "magnet_forearm_z", "problem_id"))

```

*** 

We setup a resampling control function that we will insert into each model. We use cross validation using a random forest repeated three times.

```{r presentation resample-for-all, warning=FALSE, message=FALSE, comment="", echo=TRUE}
set.seed(22)
controlTS <- rfeControl(functions=rfFuncs, method="repeatedcv", number=3)
```

We find the fields with the highest correlation factor, setting our threshold at 0.75 correlation.

```{r presentation predict-rf-feature-selection-1, warning=FALSE, message=FALSE, comment="", echo=TRUE}
set.seed(22)
correlationMatrixTS <- cor(training_nonblanks_clean[,1:44], use="complete.obs")
highlyCorrelatedTS75 <- findCorrelation(correlationMatrixTS, cutoff=0.75)
names(training_nonblanks_clean[,highlyCorrelatedTS75])
```

*** 

We use the data with all predictors to perform feature selection using the rfe function in caret. This function performs a simple backwards selection, known as recursive feature elimination (RFE). It finds the model accuracy and kappa for all predictors, and subtracts each least contributing predictor one at a time for the number of predictors we ask (20) out of all predictors.

```{r presentation predict-rf-feature-selection-2, warning=FALSE, cache=TRUE, comment="", echo=TRUE, message=FALSE}
set.seed(22)
resultsTS <- rfe(training_nonblanks_clean[,1:44], 
                 training_nonblanks_clean$classe, sizes=c(1:20), rfeControl=controlTS)
```

*** 

```{r presentation 2 predict-rf-feature-selection-2, warning=FALSE, cache=TRUE, comment="", echo=TRUE, message=FALSE}
print(resultsTS)

```

*** 

Here are the most relevant predictors and a plot of the model accuracy (for a Random Forest) as the RFE algorithm subtracts one predictor at a time from the model. By the eight predictor remove, we have model accuracy in the upper 70s percents, where it remains when additional predictors are removed from the model.

```{r presentation predict-rf-feature-selection-3, warning=FALSE, comment="", echo=TRUE, message=FALSE}
set.seed(22)
predictors(resultsTS)
```

***

```{r presentation 2 predict-rf-feature-selection-3, warning=FALSE, comment="", echo=TRUE, message=FALSE}
plot(resultsTS, type=c("g", "o"))
```
